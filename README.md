ðŸ“‹ Project OverviewThis project implements an end-to-end Natural Language Processing (NLP) pipeline designed to classify Arabic medical user queries into specific medical specialties (e.g., Internal Medicine, Dentistry, Ophthalmology).The system addresses the complexities of Arabic medical text, which often includes code-switching (mixing English medical terms with Arabic), dialectal variations, and noisy input. The solution explores a wide spectrum of approaches, ranging from classical statistical methods (TF-IDF/Naive Bayes) to Deep Learning (Bi-GRU/LSTM) and State-of-the-Art Transformers (AraBERT, AraGPT2).ðŸ”’ Data Privacy NoticeImportant: The dataset used for training and evaluating these models is sourced from Altibbi.This data is private, proprietary, and confidential.The raw CSV files referenced in the code (altibbi_specialty_data.csv) are not included in this repository.This repository serves as a showcase of the methodology, architecture, and NLP pipeline construction.ðŸ› ï¸ Methodology & Pipeline1. Exploratory Data Analysis (EDA) & CleaningSanitization: Removal of empty strings, duplicates, and URLs.English/Arabic Code-Switching Analysis: Calculated the ratio of English words per query to identify the prevalence of medical terminology in Latin script.Data Cleaning:Regex-based filtering of alphanumeric noise.Normalization of Arabic characters (unifying Alef forms, handling Teh Marbuta).2. Hybrid Translation & Localization (En $\to$ Ar)To unify the vector space, English medical terms were translated to Arabic using a multi-stage approach:Identification: Used FastText Language Identification (LID) to detect English phrases within Arabic text.Ranking: Extracted top keywords using TF-IDF to prioritize translation of information-dense terms.Machine Translation: Deployed MarianMT (Helsinki-NLP/opus-mt-en-ar) to translate detected phrases.Dictionary Mapping: Implemented a custom glossary for domain-specific entities (e.g., brand names like Voltfast, abbreviations like MRI, WBC).3. Preprocessing & Stemming ComparisonThe project benchmarked three stemming algorithms to determine the optimal normalization strategy:ISRI StemmerPorter StemmerSnowball StemmerValidation: Each stemmer was tested using a Multinomial Naive Bayes baseline.4. Semi-Supervised Label CorrectionTo improve data quality, a Semi-Supervised Learning approach was applied:A Logistic Regression model was trained on a high-confidence subset of data.The model predicted labels for the remaining dataset.Instances with high-confidence mismatches (>90%) between the original tag and the prediction were automatically corrected to reduce label noise.5. Feature Engineering & EmbeddingsMultiple text representation techniques were evaluated:Bag of Words (BoW) & TF-IDFWord2Vec (CBOW/Skip-gram)FastText (Handling out-of-vocabulary words via sub-word information)ðŸ§  Model ArchitecturesThe project progressed through three tiers of model complexity:Tier 1: Classical Machine LearningModels: Logistic Regression, Linear SVC, Multinomial & Gaussian Naive Bayes.Goal: Establish strong baselines and evaluate embedding effectiveness.Tier 2: Deep Learning (Sequential Models)Implemented using TensorFlow/Keras:Bi-Directional GRU (Gated Recurrent Unit): Tested with both Word2Vec and FastText embeddings.Bi-Directional LSTM (Long Short-Term Memory): Optimized for capturing long-range dependencies in patient narratives.Architecture: Embedding Layer (Frozen) $\to$ Bi-RNN $\to$ Dropout $\to$ Dense Layers.Tier 3: Transformers (Hugging Face / PyTorch)Fine-tuned State-of-the-Art Arabic Language Models:AraBERT (v02): BERT-based architecture optimized for Arabic segments.AraGPT2: Generative Pre-trained Transformer adapted for classification tasks.Training Details:Dynamic padding via DataCollator.Mixed Precision Training (torch.cuda.amp) for efficiency.AdamW Optimizer.ðŸ“Š Evaluation MetricsAll models were evaluated on a stratified test set using the following metrics:AccuracyWeighted F1-Score (Crucial for class imbalance)Precision & RecallðŸ’» Tech StackLanguages: Python 3.xData Manipulation: Pandas, NumPy, RegexNLP Tools: NLTK, Gensim, FastTextMachine Learning: Scikit-LearnDeep Learning: PyTorch (Transformers), TensorFlow/Keras (RNNs)Translation: Hugging Face Pipelines (MarianMT)ðŸš€ UsageTo run the pipeline (assuming you have access to your own dataset formatted similarly):Install Dependencies:Bashpip install numpy pandas nltk scikit-learn gensim fasttext torch tensorflow transformers datasets evaluate
Download NLTK Data:Pythonimport nltk
nltk.download('punkt')
nltk.download('stopwords')
Run the Notebook/Script:Execute the processing blocks sequentially. Ensure the path to the CSV file is updated to your local directory.
